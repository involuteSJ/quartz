## ==목차==

1. Face recognition with LBPH and OpenCV
2. Face recognition with Dlib, CNN and distance calculation
3. Face recognition using the webcam

  

## ==LBPH (Local Binary Patterns Histograms)==

지역 이진 패턴 히스토그램

1. 이미지 행렬 분석

![[90. Settings/92. Attachments/Untitled 25.png|Untitled 25.png]]

if ≥ 8 : 1

if < 8 : 0

Binary : 11100010

Decimal : 226

![[90. Settings/92. Attachments/Untitled 1 7.png|Untitled 1 7.png]]

행렬의 중심인 8을 기준으로 계산

좌측 사진을 8을 기준으로 8이상이면 1, 아니면 0으로 계산한 결과 오른쪽과 같은 행렬이 나옴

Binary 로 변환하면 좌측 상단인 1부터 시계방향으로 차례대로 적어서 11100010 이 나옴

Binary 숫자를 10진수로 바꾸면 226이라는 결과값이 나오는데, 이건 중앙을 둘러싼 픽셀의 개수가 226개라는 것을 의미함

행렬의 값이 클수록 밝다는 것을 의

1. 얼굴 분석

얼굴을 일정한 격자로 자르고 각각의 격자에는 행렬 분석에 대한 정보들이 포함되어 있음

각각의 격자에는 3*4의 작은 격자가 추가로 들어있음

모든 격자들의 정보를 각각 히스토그램으로 나타내면 얼굴의 가장자리를 인식할 수 있음

![[be8763e3-019d-4f56-b1eb-0e729ba07947.png]]

  

실습

```Python
from PIL import Image
import cv2
import numpy as np

\#압축 파일을 해제할때 사용
import zipfile
path='경로'
zip_object = zipfile.ZipFile(file=path, mode='r')
zip_object.extractall('./')
zip_object.close()
```

전처리 과정

```Python
import os

def get_image_data():
	\#이미지 경로와 파일명을 합치는 과정 ex) '/content/yalefaces/train/subject02.surprised.gif'
	paths = [os.path.join('학습시킬 사진이 들어있는 폴더', f) for f in os.listdir('앞에서 사용한 경로')]
	\#이미지 정보 저장
	faces = []
	\#클래스 이름 저장
	ids = []
	for path in paths:
		\#L : 이미지 모드 (흑백)
		image = Image.open(path).convert('L')
		\#image를 OpenCV에서 사용하기 위해 numpy 배열로 변환
		image_np = np.array(image, 'uint8')
		\#파일 경로에서 파일명에 있는 id 값을 가져오는 과정
		id = int(os.path.split(path)[1].split('.')[0].replace('subject',''))
		
		\#ids : 모든 face에 대한 id
		ids.append(id)
		\#faces : 각각 사진에 대한 모든 픽셀 값 저장
		faces.append(image_np)

	return np.array(ids), faces
```

LBPH 분류기 학습

```Python
\#사진을 8행 8열로 나누어 총 64개의 히스토그램을 저장
lbph_classifier = cv2.face.LBPHFaceRecognizer_create()
\#LBPH 알고리즘을 이용해서 학습 실행
lbph_classifier.train(faces, ids)
\#학습 완료 자료 저장
lbph_classifier.write('lbph_classifier.yml')
```

얼굴 인식

```Python
\#학습한 얼굴인식 적용
lbph_face_classifier = cv2.face.LBPHFaceRecognizer_create()
lbph_face_classifier.read('학습한 파일')

\#테스트 진행 전처리
test_image = '테스트할 사진 경로'
image = Image.open(test_image).convert('L')
image_np = np.array(image, 'uint8')

\#예측 실행
prediction = lbph_face_classifier.predict(image_np)
\#예상되는 출력값
expected_output = int(os.path.split(test_image)[1].split('.')[0].replace('subject','')
\#글씨 작성후 사진 출력
cv2.putText(image_np, 'Pred: ' + str(prediction[0]),(10,30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0,255,0))
cv2.putText(image_np, 'Exp: ' + str(expected_output),(10,30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0,255,0))
cv2_imshow(image_np)
```

![[90. Settings/92. Attachments/Untitled 2 8.png|Untitled 2 8.png]]

  

얼굴 분류기 평가

```Python
paths = [os.path.join('테스트 파일 경로', f) for f in os.listdir('앞에 적은 경로')]
\#예측과 결과 출력 배열
predictions = []
expected_outputs = []
\#예측 사진들 전처리 과정
for path in paths:
	image = Image.open(path).convert('L')
	image_np = np.array(image, 'uint8')
	prediction, _ = lbph_face_classifier.predict(image_np)
	expected_output = int(os.path.split(path)[1].split('.')[0].replace('subject','')
	
	predictions.append(prediction)
	expected_outputs.append(expected_output)
	
\#list 값들을 numpy 배열로 변환
predictions = np.array(predictions)
expected_outputs = np.array(expected_outputs)

\#정확하게 예측한 값 분석
from sklearn.metrics import accuracy_score
accuracy_score(expected_outputs, predictions)

\#오차행렬
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(expected_outputs, predictions)
cm

import seaborn
seaborn.heatmap(cm, annot=True)
```

  

LBPH 매개변수

/

---

Radius(범위)

---

Neighbors (사용되는 픽셀의 수)

---

grid_x (가로축 셀의 개수)

grid_y (세로축 셀의 개수)

---

Threshold (임계값) : 신뢰도

장점

---

이미지에서 더 많은 패턴 발견 가능

---

  

---

  

---

값이 높을수록 품질이 올라감

  

단점

---

모서리 발견에 어려움을 겪을 가능성 있음

---

  

---

  

---

  

LBPH 매개변수를 이용한 분석 정확도 향상

```Python
# 매개변수들의 기본값
# threshold : 1.7976931348623157e+308
# radius : 1
# neighbors : 8
# grid_x : 8
# grid_y : 8

# 매개변수 값 변경후 분류기 생성
lbph_classifier = cv2.face.LBPHFaceRecognizer_create(radius=4, neighbors=14, grid_x=9, grid_y=9)
# 생성한 분류기를 이용해서 훈련
lbph_classifier.train(faces, ids)
# 훈련시킨 분류기 작성
lbph_classifier.write('lbph_classifier.yml')

# 분류기 생성
lbph_face_classifier = cv2.face.LBPHFaceRecognizer_create()
lbph_face_classifier.read('생성한 분류기 경로')

# 테스트 파일 전처리
paths = [os.path.join('테스트 파일 경로', f) for f in os.listdir('테스트 파일 경로')]
predictions = []
expected_outputs = []
for path in paths:
	image = Image.open(path).convert('L')
	image_np = np.array(image, 'uint8')
	prediction, _ = lbph_face_classifier.predict(image_np)
	expected_output = int(os.path.split(path)[1].split('.')[0].replace('subject',''))
	
	predictions.append(prediction)
	expected_outputs.append(expected_output)
	
# numpy 배열로 변환
predictions = np.array(predictions)
expected_outputs = np.array(expected_outputs)

# 정확도 계산
from sklearn.metrics import accuracy_score
accuracy_score(expected_outputs, predictions)
```

  

Dlib 라이브러리를 이용한 얼굴 인식

```Python
import dlib
import cv2
\#colab에서 진행시
from google.colab.patches import cv2_imshow
```

  

Detecting facial points (얼굴 포인트 감지 방법)

```Python
# 얼굴 감지와 동일한 함수
face_detector = dlib.get_frontal_face_detector()
# 68개의 포인트를 이용해 얼굴 감지
points_detector = dlib.shape_predictor('얼굴 감지에 사용될 학습 모델')

image = cv2.imread('판단할 이미지 자료')
face_detection = face_detector(image, 1)
for face in face_detection:
	# 이미지에서 얼굴에 해당하는 포인트를 감지해서 초록색 원을 그림
	points = points_detector(image, face)
	for point in points.parts():
		cv2.circle(image, (point.x, point.y), 2, (0, 255, 0), 1)

	l, t, r, b = face.left(), face.top(), face.right(), face.bottom()
	cv2.rectangle(image, (l, t), (r, b), (0, 255, 255), 2)
cv2_imshow(image)
```

  

Detecting facial descriptors (얼굴 기술자 감지)

```Python
import os
# 얼굴 감지와 동일한 함수
face_detector = dlib.get_frontal_face_detector()
# 68개의 포인트를 이용해 얼굴 감지
points_detector = dlib.shape_predictor('얼굴 감지에 사용될 학습 모델')
face_descriptor_extractor = dlib.face_recognition_model_v1('사전 학습된 파일 ')

# 변수 정의
index = {}
idx = 0
face_descriptors = None

path = [os.path.join('학습시킬 사진', f) for f in os.listdir('학습시킬 사진')]
image = Image.open(path).convert('RGB')
image_np = np.array(image, 'uint8')
face_detection = face_detector(image_np, 1)
for face in face_detection:
	l, t, r, b = face.left(), face.top(), face.right(), face.bottom()
	cv2.rectangle(image_np, (l, t), (r, b), (0, 0, 255), 2)
	
	points = points_detector(image_np, face)
	for point in points.parts():
		cv2.circle(image_np, (point.x, point.y), 2, (0, 255, 0), 1)
		
	# 합성곱 신경망 알고리즘에 이미지 전체와 얼굴로 인식한 point (68개)를 전달 후 128개 값 출력
	face_descriptor = face_descriptor_extractor.compute_face_descriptor(image_np, points)
	# 각 얼굴의 특징이 저장된 값들을 list 형식으로 저장
	face_descriptor = [f for f in face_descriptor]
	# np.float64 타입으로 변환후 저장 (쉼표가 없음)
	face_descriptor = np.asarray(face_descriptor, dtype=np.float64)
	# 콜론 : 이 배열에 들어있는 모든 값을 대상으로 함
	# 벡터 -> 1*128 행렬로 변경
	face_descriptor = face_descriptor[np.newaxis, :]
	
	# face_descriptors에 모든 얼굴 정보 저장
	if face_descriptors is None:
		face_descriptors = face_descriptor
	else :
		face_descriptors = np.concatenate((face_descriptors, face_descriptor), axis = 0)
	
	# index는 파일 정보 관리
	index[idx] = path
	idx += 1
	
\#cv2.imshow(image_np)
```

![[90. Settings/92. Attachments/Untitled 3 6.png|Untitled 3 6.png]]

  

  

Calculating the distance between faces

```Python
# 거리 값이 낮을수록 유사도가 높음

# linalg : 선형대수
# 거리 혹은 유사도 계산
np.linalg.norm(face_descriptors[131] - face_discriptors[131])
# 0번 배열에 있는 얼굴 값과 배열에 있는 모든 값들을 비교
np.linalg.norm(face_descriptors[0] - face_descriptros, axis = 1)
# 0번 배열의 얼굴 값과 배열의 모든 값들을 비교 후 가장 작은 값의 배열번호를 출력
np.argmin(np.linalg.norm(face_descriptors[0] - face_descriptors[1:], axis = 1))
```

  

Detecting faces with Dlib

```Python
# threshold 변수로 얼굴의 유사도가 어느정도의 값이어야 동일인물로 판단할지 결정
threshold = 0.5
predictions = []
expected_outputs = []

paths = [os.path.join('테스트 파일 경로', f) for f in os.listdir('테스트 파일 경로')]
for path in paths:
	image = Image.open(path).convert('RGB')
	image_np = np.array(image, 'uint8')
	face_detection = face_detector(image_np, 1)
	for face in face_detection:
		points = points_detector(image_np, face)
		face_descriptor = face_descriptor_extractor.compute_face_descriptor(image_np, points)
		face_descriptor = [f for f in face_descriptor]
		face_descriptor = np.asarray(face_descriptor, dtype=np.float64)
		face_descriptor = face_descriptor[np.newaxis, :]
		
		# 유사도 계산
		distances = np.linalg.norm(face_descriptor - face_descriptors, axis = 1)
		min_index = np.argmin(distances)
		min_distance = distances[min_index]
		# 유사도 값이 threshold값보다 작으면 본인이라 판단
		if min_distance <= threshold:
			name_pred = int(os.path.split(index[min_index])[1].split('.')[0].replace('subject', ''))
		else:
			name_pred = 'Not identified'
			
		name_real = int(os.path.split(index[min_index])[1].split('.')[0].replace('subject', ''))
		
		predictions.append(name_pred)
		expected_outputs.append(name_real)
		
		# 사진에 예측값과 실제값 출력
		cv2.putText(image_np, 'Pred: ' + str(name_pred), (10, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 255, 0))
		cv2.putText(image_np, 'Exp: ' + str(name_real), (10, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 255, 0))
		
	cv2.imshow(image_np)
	
predictions = np.array(predictions)
expected_outputs = np.array(expected_outputs)

from sklearn.metrics import accuracy_score
accuracy_score(expected_outputs, predictions)
```